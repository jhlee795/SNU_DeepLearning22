{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# M2177.003100 Deep Learning <br> Final Project: Text-to-Image Synthesis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright (C) Data Science & AI Laboratory, Seoul National University. This material is for educational uses only. Some contents are based on the material provided by other paper/book authors and may be copyrighted by them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**For understanding of this work, please carefully look at given PDF file.**\n",
    "\n",
    "**Note**: certain details are missing or ambiguous on purpose, in order to test your knowledge on the related materials. However, if you really feel that something essential is missing and cannot proceed to the next step, then contact the teaching staff with clear description of your problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jaehyun/anaconda3/envs/geometry/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2022-12-19 15:38:47.324031: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-12-19 15:38:47.404230: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2022-12-19 15:38:47.737384: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.3/lib64\n",
      "2022-12-19 15:38:47.737428: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/usr/local/cuda-11.3/lib64\n",
      "2022-12-19 15:38:47.737431: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('/home/jaehyun/Downloads/22Final/')\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from models import GENERATOR\n",
    "from config import cfg, cfg_from_file\n",
    "import pprint\n",
    "import datetime\n",
    "import dateutil.tz\n",
    "import time\n",
    "from PIL import Image\n",
    "\n",
    "from encoders import CNN_ENCODER, RNN_ENCODER\n",
    "from dataset import CUBDataset\n",
    "from utils import *\n",
    "from loss import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using config:\n",
      "{'BATCH_SIZE': 10,\n",
      " 'CA': False,\n",
      " 'CHECKPOINT_DIR': './checkpoints',\n",
      " 'CHECKPOINT_NAME': '',\n",
      " 'CNN': {'EMBEDDING_DIM': 32, 'H_DIM': 256},\n",
      " 'CONFIG_NAME': 'text-to-image',\n",
      " 'CUDA': True,\n",
      " 'DATASET_NAME': 'birds',\n",
      " 'DATA_DIR': 'data/birds',\n",
      " 'EMBEDDING_TYPE': 'cnn-rnn',\n",
      " 'GAN': {'B_ATTENTION': False,\n",
      "         'B_CONDITION': False,\n",
      "         'B_DCGAN': False,\n",
      "         'CONDITION_DIM': 100,\n",
      "         'DF_DIM': 32,\n",
      "         'EMBEDDING_DIM': 256,\n",
      "         'GF_DIM': 64,\n",
      "         'R_NUM': 2,\n",
      "         'Z_DIM': 100},\n",
      " 'GPU_ID': '0',\n",
      " 'IMAGE_SIZE': 128,\n",
      " 'NUM_BATCH_FOR_TEST': 4,\n",
      " 'RANDOM_SEED': 1234,\n",
      " 'RNN': {'EMBEDDING_DIM': 0,\n",
      "         'H_DIM': 0,\n",
      "         'TYPE': 'LSTM',\n",
      "         'VOCAB_SIZE': 0,\n",
      "         'WORD_EMBEDDING_DIM': 0},\n",
      " 'R_PRECISION_DIR': '',\n",
      " 'R_PRECISION_FILE': '',\n",
      " 'R_PRECISION_FILE_HIDDEN': '',\n",
      " 'TEST': {'B_EXAMPLE': False,\n",
      "          'GENERATED_HIDDEN_TEST_IMAGES': '',\n",
      "          'GENERATED_TEST_IMAGES': './evaluation/generated_images'},\n",
      " 'TEXT': {'CAPTIONS_PER_IMAGE': 10, 'EMBEDDING_DIM': 256, 'WORDS_NUM': 18},\n",
      " 'TRAIN': {'B_NET_D': False,\n",
      "           'CNN_ENCODER': '',\n",
      "           'COEFF': {'COLOR_LOSS': 1.0, 'KL': 2.0, 'UNCOND_LOSS': 1.0},\n",
      "           'DISCRIMINATOR': '',\n",
      "           'DISCRIMINATOR_LR': 0.0002,\n",
      "           'FLAG': False,\n",
      "           'GENERATOR': '',\n",
      "           'GENERATOR_LR': 0.0002,\n",
      "           'MAX_EPOCH': 600,\n",
      "           'RNN_ENCODER': '',\n",
      "           'SMOOTH': {'GAMMA1': 0.0,\n",
      "                      'GAMMA2': 0.0,\n",
      "                      'GAMMA3': 0.0,\n",
      "                      'LAMBDA': 0.0},\n",
      "           'SNAPSHOT_INTERVAL': 50},\n",
      " 'TREE': {'BASE_SIZE': 32, 'BRANCH_NUM': 2},\n",
      " 'WORKERS': 4,\n",
      " 'WRONG_CAPTION': 9}\n"
     ]
    }
   ],
   "source": [
    "# Set a config file as 'train_birds.yml' in training, as 'eval_birds.yml' for evaluation\n",
    "cfg_from_file('cfg/eval_birds.yml') # eval_birds.yml\n",
    "\n",
    "print('Using config:')\n",
    "pprint.pprint(cfg)\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = cfg.GPU_ID\n",
    "\n",
    "now = datetime.datetime.now(dateutil.tz.tzlocal())\n",
    "timestamp = now.strftime('%Y_%m_%d_%H_%M_%S')\n",
    "output_dir = 'sample/%s_%s_%s' % (cfg.DATASET_NAME, cfg.CONFIG_NAME, timestamp)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.current_dir:\n",
      "/home/jaehyun/Downloads/22Final\n",
      "\n",
      "self.data_dir:\n",
      "/home/jaehyun/Downloads/22Final/data/birds\n",
      "\n",
      "self.image_dir:\n",
      "/home/jaehyun/Downloads/22Final/data/birds/CUB-200-2011/images\n",
      "\n",
      "filepath /home/jaehyun/Downloads/22Final/data/birds/captions.pickle\n",
      "Load from:  /home/jaehyun/Downloads/22Final/data/birds/captions.pickle\n",
      "test data directory:\n",
      "/home/jaehyun/Downloads/22Final/data/birds/test\n",
      "\n",
      "# of test filenames:(2933,)\n",
      "\n",
      "example of filename of valid image:001.Black_footed_Albatross/Black_Footed_Albatross_0046_18\n",
      "\n",
      "example of caption and its ids:\n",
      "['this', 'is', 'a', 'small', 'bird', 'that', 'has', 'a', 'brilliant', 'blue', 'color', 'on', 'it', 's', 'body', 'a', 'slightly', 'darker', 'blue', 'on', 'it', 's', 'head', 'a', 'teal', 'color', 'on', 'it', 's', 'wings', 'and', 'a', 'light', 'colored', 'beak']\n",
      "[18, 19, 1, 250, 2, 33, 13, 1, 853, 50, 37, 86, 53, 54, 15, 1, 178, 31, 50, 86, 53, 54, 25, 1, 1054, 37, 86, 53, 54, 17, 8, 1, 67, 89, 10]\n",
      "\n",
      "# of test captions:(29330,)\n",
      "\n",
      "# of test caption ids:(29330,)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_268026/3617630386.py:14: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(f'# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
      "/tmp/ipykernel_268026/3617630386.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  print(f'# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')\n"
     ]
    }
   ],
   "source": [
    "imsize = cfg.TREE.BASE_SIZE * (4 ** (cfg.TREE.BRANCH_NUM - 1))\n",
    "image_transform = transforms.Compose([\n",
    "    transforms.Resize(int(imsize)),\n",
    "    transforms.RandomCrop(imsize),\n",
    "    transforms.RandomHorizontalFlip()])\n",
    "\n",
    "#### Load the dataset\n",
    "test_dataset = CUBDataset(cfg.DATA_DIR, transform=image_transform, split='test')\n",
    "\n",
    "print(f'test data directory:\\n{test_dataset.split_dir}\\n')\n",
    "print(f'# of test filenames:{test_dataset.filenames.shape}\\n')\n",
    "print(f'example of filename of valid image:{test_dataset.filenames[0]}\\n')\n",
    "print(f'example of caption and its ids:\\n{test_dataset.captions[0]}\\n{test_dataset.captions_ids[0]}\\n')\n",
    "print(f'# of test captions:{np.asarray(test_dataset.captions).shape}\\n')\n",
    "print(f'# of test caption ids:{np.asarray(test_dataset.captions_ids).shape}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE, drop_last=True, shuffle=False, num_workers=int(cfg.WORKERS),\n",
    "                                              collate_fn = my_collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define models and load weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: './checkpoints/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 10\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m#######################################################\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# TODO\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# (1) Build Generator and Network\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m######################################################\u001b[39;00m\n\u001b[1;32m      8\u001b[0m netG \u001b[38;5;241m=\u001b[39m GENERATOR()\n\u001b[0;32m---> 10\u001b[0m state_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cfg\u001b[38;5;241m.\u001b[39mCHECKPOINT_DIR, cfg\u001b[38;5;241m.\u001b[39mTRAIN\u001b[38;5;241m.\u001b[39mGENERATOR), map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m storage, loc: storage)\n\u001b[1;32m     11\u001b[0m text_encoder\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_encoder\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m text_encoder\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/anaconda3/envs/geometry/lib/python3.9/site-packages/torch/serialization.py:699\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m pickle_load_args\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    697\u001b[0m     pickle_load_args[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 699\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_open_file_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m opened_file:\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_zipfile(opened_file):\n\u001b[1;32m    701\u001b[0m         \u001b[38;5;66;03m# The zipfile reader is going to advance the current file position.\u001b[39;00m\n\u001b[1;32m    702\u001b[0m         \u001b[38;5;66;03m# If we want to actually tail call to torch.jit.load, we need to\u001b[39;00m\n\u001b[1;32m    703\u001b[0m         \u001b[38;5;66;03m# reset back to the original position.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m         orig_position \u001b[38;5;241m=\u001b[39m opened_file\u001b[38;5;241m.\u001b[39mtell()\n",
      "File \u001b[0;32m~/anaconda3/envs/geometry/lib/python3.9/site-packages/torch/serialization.py:230\u001b[0m, in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_open_file_like\u001b[39m(name_or_buffer, mode):\n\u001b[1;32m    229\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_path(name_or_buffer):\n\u001b[0;32m--> 230\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_open_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m mode:\n",
      "File \u001b[0;32m~/anaconda3/envs/geometry/lib/python3.9/site-packages/torch/serialization.py:211\u001b[0m, in \u001b[0;36m_open_file.__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, name, mode):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28msuper\u001b[39m(_open_file, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mIsADirectoryError\u001b[0m: [Errno 21] Is a directory: './checkpoints/'"
     ]
    }
   ],
   "source": [
    "# load the text encoder model to generate images for evaluation\n",
    "text_encoder = RNN_ENCODER(test_dataset.n_words, nhidden=cfg.TEXT.EMBEDDING_DIM)\n",
    "\n",
    "#######################################################\n",
    "# TODO\n",
    "# (1) Build Generator and Network\n",
    "######################################################\n",
    "netG = GENERATOR()\n",
    "\n",
    "state_dict = torch.load(os.path.join(cfg.CHECKPOINT_DIR, cfg.TRAIN.GENERATOR), map_location=lambda storage, loc: storage)\n",
    "text_encoder.load_state_dict(state_dict['text_encoder'])\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "netG.load_state_dict(state_dict['netG'])\n",
    "for p in netG.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print('Load model from:', os.path.join(cfg.CHECKPOINT_DIR, cfg.TRAIN.GENERATOR))\n",
    "text_encoder.eval()\n",
    "netG.eval()\n",
    "\n",
    "bs = cfg.BATCH_SIZE\n",
    "noise = Variable(torch.FloatTensor(bs, cfg.GAN.Z_DIM))\n",
    "\n",
    "if cfg.CUDA:\n",
    "    text_encoder = text_encoder.cuda()\n",
    "    netG = netG.cuda()\n",
    "    noise = noise.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Generate Images and save them to evaluate metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, data in enumerate(test_dataloader, 0):\n",
    "    _, captions, cap_lens, class_ids, keys, sent_idx = prepare_data(data)\n",
    "    \n",
    "    #######################################################\n",
    "    # TODO\n",
    "    # (2) Compute Text Embeddings using RNN Encoder\n",
    "    ######################################################\n",
    "\n",
    "    #######################################################\n",
    "    # TODO\n",
    "    # (3) Generate fake images using Generator\n",
    "    ######################################################\n",
    "    noise.data.normal_(0, 1)\n",
    "\n",
    "    fake_imgs = None\n",
    "\n",
    "    for j in range(bs):\n",
    "        if not os.path.exists(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, keys[j].split('/')[0])):\n",
    "            os.makedirs(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, keys[j].split('/')[0]))\n",
    "\n",
    "        im = fake_imgs[j].data.cpu().numpy()\n",
    "        im = (im + 1.0) * 127.5\n",
    "        im = im.astype(np.uint8)\n",
    "        im = np.transpose(im, (1, 2, 0))\n",
    "        im = Image.fromarray(im)\n",
    "        print(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, keys[j] + '_{}.png'.format(sent_idx[j])))\n",
    "        im.save(os.path.join(cfg.TEST.GENERATED_TEST_IMAGES, keys[j] + '_{}.png'.format(sent_idx[j])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geometry",
   "language": "python",
   "name": "geometry"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "07d22452b20408ffb63ff7af541ea70f1482b1754eeab403cb7a35610db14220"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
